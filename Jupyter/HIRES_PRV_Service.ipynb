{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keck HIRES Precision Radial Velocity Data Reduction \n",
    "\n",
    "<b><i>This service enables reduction and analysis of precision radial velocity (PRV) data from the HIRES Keck instrument. </i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b><font style=\"color: red;\">This notebook is not part of the official release, not even for alpha testing.  The project found it useful for its own testing but makes no guarantees it will even work for anyone else.  Use, and even read, at your own risk.</font></b></i>\n",
    "\n",
    "This notebook introduces the Keck HIRES Precision Radial Velocity (PRV) pipeline service and works through one specific example. There are number of variations, mostly having to do with planning processing, which will be covered in more detail by other notebooks but here you will see all the basics.\n",
    "\n",
    "The notebook is kept with the HIRES PRV Python access toolkit in <a href=\"https://github.com/Caltech-IPAC/hiresprv\">GitHub: https://github.com/Caltech-IPAC/hiresprv</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login\n",
    "\n",
    "Logging in the first time creates a workspace for the user and associates it with a KOA account.\n",
    "\n",
    "Users of this service must have Keck On-line Archive (KOA) accounts and use that login here to gain access to their data.  Even researchers planning to use only public data will need a KOA login as this service is maintaining persistent storage under that ID.\n",
    "\n",
    "The login is persisted through the use of HTTP cookies and logging in from multiple clients will connect the user to the same account, storage, and processing history.  This environment (user workspace) is permanent as we expect some on-going research to span years.  The login for a give client machine need only be done once, assuming the cookie file in the local storage is not deleted.  If it is, logging in again will reconstruct it.\n",
    "\n",
    "The cookie file, processing state information, and downloaded results like 1D spectra and RV curve tables will be kept locally in the same space as this notebook.  If you wish to change that, simply add in whatever directory management and navigation you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "from hiresprv.auth import login\n",
    "\n",
    "login('prv.cookies')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOA Data Retrieval\n",
    "\n",
    "The PRV workspace first needs to be populated with data from the KOA Archive.  This can be done all at once, if the data exists, or incrementally as the data is taken/identified.\n",
    "\n",
    "This step is more than a simple data transfer.  \"Raw reduction\" of the data, which converts the 2D CCD spectal images to 1D spectra, is done up-front as the data is retrieved a night at a time.  The UT dates you give here are actually shifted a few hours to catch any calibration data collected in the afternoon of the same (Hawaii-local) day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiresprv.archive import Archive\n",
    "\n",
    "koa = Archive('prv.cookies')\n",
    "\n",
    "rtn = koa.by_dates(\"\"\"2009-12-31\n",
    "2013-06-29\n",
    "2013-09-12\n",
    "2014-06-06\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note that since the data in the workspace is permanent, repeated request for the same data would not change anything and so those dates will be ignored.  Therefore, you can add to the above list or replace it with new dates as you choose.  Dates must be formatted YYYY-MM-DD (a KOA requirement).\n",
    "\n",
    "The above service responds immediately with an acknowledgement of the request and starts the actual transfer and raw data reduction (which can take some time) as a background job.  The job status can be checked by polling or can be monitored using the function below.  While one retrieval job (or processing job below) is running, no others can be initiated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRV Processing Monitor\n",
    "\n",
    "The PRV monitor makes use of IFrames.  An IFrame is a region in the Jupyter page which can display another full browser document, in this case a real-time web service watching the workspace status file.  Unlike the rest of the Jupyter page, where we have very little control over layout and event management, the IFrame contents can be treated much more like a traditional HTML/Javascript GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiresprv.status import Status\n",
    "\n",
    "monitor = Status('prv.cookies')\n",
    "\n",
    "url = monitor.processing_status()\n",
    "\n",
    "IFrame(url, 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "Once data has been retrieve and the \"nightly\" raw reduction performed, a set of records is added to a persistent metadata table, one per observation.  These observations are all taken through the HIRES PRV instrument (2D CCD) and will have been reduced to 1D spectra by the raw reduction.  They fall into five classes:\n",
    "\n",
    "<ul>\n",
    "<li><b>RV observations</b> -- Multiple observations of a star with the iodine cell in the light path.  These are the measurements that each actually result in a calculation of a radial velocity.<p/></li>\n",
    "    \n",
    "<li><b>Templates</b> -- One long observation of the same star without iodine, for reference.<p/></li>\n",
    "    \n",
    "<li><b>B stars</b> -- A set of observations of rapidly rotating B stars bracketing the template observation and used to reduce it.<p/></li>\n",
    "    \n",
    "<li><b>Iodine</b> -- Reference observation of iodine for nightly calibration.<p/></li>\n",
    "    \n",
    "<li>Miscellaneous other calibration observations (labelled as \"<b>Unknown</b>\").<p/></li>\n",
    "</ul>\n",
    "\n",
    "By inspecting this table, the user can determine what objects were observed, whether there are template observation for which of them (and adequate B star data to reduce a template), and whether there are enough RV measurements to generate a final RV curve.\n",
    "\n",
    "With a small metadata table, this is simple enough to do by inspection but a typical workspace can easily have thousands of files covering tens or hundreds of objects.  Furthermore, since observations for a single object are frequently spread out over years, the metadata table is often fairly thorougly mixed in time.\n",
    "\n",
    "Luckily, there are a number of tools available in client-side Python subset and organize the metadata, so we provide it for download as a CSV table or an SQLite binary file or even, as here, as a simple HTML table.  The workspace copy of the data is maintained in an SQLite database so we also provide a basic filtering mechanism as an optional addition to the download.  This filtering is often adequate for basic processing scenarios.  \n",
    "\n",
    "Note that metadata retrieval can't be done while the system is \"busy\" (downloading additional data or further reducing data in the workspace). Otherwise, metadata downloads can be done at any time.\n",
    "\n",
    "Also note that the client-side file will become out-of-date once new data download or processing requests are submitted.  It is up to the user to re-request the new metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiresprv.database import Database\n",
    "\n",
    "state = Database('prv.cookies')\n",
    "\n",
    "url = state.search()\n",
    "\n",
    "IFrame(url, 1000,  500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting the Metadata: Single Target\n",
    "\n",
    "Ultimately, to make an RV curve for one star we need to reduce its observations into RV measurements.  Assuming there are adequate B star observations to reduce the template, we can isolate appropriate records in the metadata above by simply filtering on TARGET name.  There are many ways to do this; in our case we we used the remote SQLite query capability and filtered it with <p/>\n",
    "\n",
    "<tt>select DATE, OBTYPE, FILENAME, TARGET, BJD, BCVEL from FILES where TARGET like 'HD185144';</tt> \n",
    "\n",
    "The resulting data is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = state.search(sql=\"select DATE, OBTYPE, FILENAME, TARGET, BJD, BCVEL from FILES where TARGET like 'HD185144';\")\n",
    "\n",
    "IFrame(url, 700,  325)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RV Pipeline Processing\n",
    "\n",
    "This shows that on 12/31/2009 three separate RV observations were made of HD185144 followed by five template observations (which the pipeline will combine into a single template).  Five years later, another three RV observations were made.\n",
    "\n",
    "As with the data download, the further reduction steps in the pipeline can be quite lengthy (minutes to hours each), so rather than have the user monitor each one, we provide a scripting mechanism so complex reduction jobs can be submitted in on shot.\n",
    "\n",
    "In order to turn any of the RV observations into an RV value, we need the template.  So we will generate that first.  Since it is possible to repeat the template observations on more than one day, we need to explicitly state which object and which day.  The script command for this is:\n",
    "\n",
    "<pre>template HD185144 20091231</pre>\n",
    "\n",
    "To reduce an RV measurement, we have to refer to this template (the target name is enough) and specify which file to reduce.  For example:\n",
    "\n",
    "<pre>rv HD185144 r20091231.7</pre>\n",
    "\n",
    "Finally, once we have a set of RV measurements for an object, we a generate an RV curve (the pipeline finds all the appropriate RV measurements):\n",
    "\n",
    "<pre>rvcurve HD185144</pre>\n",
    "\n",
    "As long as we follow the general rules that we need a template before we can reduce an RV measurement and we need at least three RV measurements before we can generate an RV curve, we can otherwise scripte things in whatever order we wish (<i>e.g.</i> all the templates first).\n",
    "\n",
    "All of this is submitted to the pipeline as a text script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiresprv.idldriver import Idldriver\n",
    "\n",
    "idl = Idldriver('prv.cookies')\n",
    "\n",
    "rtn = idl.run_script(\"\"\"\n",
    "template HD185144 20091231\n",
    "rv HD185144 r2R0091231.72\n",
    "rv HD185144 r20091231.73\n",
    "rv HD185144 r20091231.74\n",
    "rv HD185144 r20150606.145\n",
    "rv HD185144 r20150606.146\n",
    "rv HD185144 r20150606.147\n",
    "rvcurve HD185144\n",
    "\"\"\")\n",
    "\n",
    "print(rtn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring (again)\n",
    "\n",
    "To monitor the pipeline processing request, you can either insert another cell the runs the PRV Processing Monitor here or just go back to the previous cell and monitor from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Retrieval\n",
    "\n",
    "The principal retrievable items are the RV curves (CSV files) for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  rvdata = prvDataRetrieval(target='HD185144')\n",
    "#  print(rvdata)\n",
    "\n",
    "url='HD185144.txt'\n",
    "IFrame(url, 700,  325)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
